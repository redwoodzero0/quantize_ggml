{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["WqI1CPiXI4dP"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["* `q2_k`: attention.vw 및 feed_forward.w2 텐서에는 Q4_K를 사용하고, 그 외 텐서에는 Q2_K를 사용합니다.\n","* `q3_k_l`: attention.wv, attention.wo, feed_forward.w2 텐서에는 Q5_K를 사용하고, 그 외 텐서에는 Q3_K를 사용합니다.\n","* `q3_k_m`: attention.wv, attention.wo, feed_forward.w2 텐서에는 Q4_K를 사용하고, 그 외에는 Q3_K를 사용합니다.\n","* `q3_k_s`: 모든 텐서에 Q3_K 사용\n","* `q4_0`: 기본 양자화 방식, 4비트.\n","* `q4_1`: q4_0보다 정확도가 높지만 q5_0만큼 높지는 않습니다.그러나 q5 모델보다 추론 속도가 빠릅니다.\n","* `q4_k_m`: attention.wv 및 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 그 외에는 Q4_K ~를 사용합니다.\n","* `q4_k_s`: 모든 텐서에 Q4_K 사용\n","* `q5_0`: 정확도가 높고, 리소스 사용량이 많으며, 추론 속도가 느립니다.\n","* `q5_1`: 더 높은 정확도, 더 높은 리소스 사용량, 더 느린 추론.\n","* `q5_k_m`: attention.wv 및 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 그 외에는 Q5_K를 사용합니다.~\n","* `q5_k_s`: 모든 텐서에서 Q5_K 사용\n","* `q6_k`: 모든 텐서에 Q6_K 사용\n","* `q8_0`: float16과 거의 구별할 수 없음.리소스 사용량이 많고 느립니다.대부분의 사용에는 권장되지 않습니다."],"metadata":{"id":"8y_Rk94LzG7I"}},{"cell_type":"code","source":["MODEL_ID = \"Redwood0/PiVoT-Merge-A-7B\"\n","QUANTIZATION_METHODS = [\"q5_k_m\"]\n","\n","MODEL_NAME = MODEL_ID.split('/')[-1]\n","GGML_VERSION = \"gguf\"\n","\n","# Install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n","!pip install -r llama.cpp/requirements.txt"],"metadata":{"id":"fD24jJxq7t3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download model\n","!git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}"],"metadata":{"id":"1Xav5FDgXqz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Convert to fp16\n","fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.fp16.bin\"\n","!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","\n","# Quantize the model for each method in the QUANTIZATION_METHODS list\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}"],"metadata":{"id":"poKVWiZjW9pN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 추론 실행\n","\n","다음은 양자화된 모델을 실행하는 간단한 스크립트입니다.추론 속도를 높이기 위해 모든 레이어를 GPU로 오프로드(7b 매개 변수 모델의 경우 35개)하고 있습니다."],"metadata":{"id":"WqI1CPiXI4dP"}},{"cell_type":"code","source":["import os\n","\n","model_list = [file for file in os.listdir(MODEL_NAME) if GGML_VERSION in file]\n","\n","prompt = input(\"Enter your prompt: \")\n","chosen_method = input(\"Please specify the quantization method to run the model (options: \" + \", \".join(model_list) + \"): \")\n","\n","# Verify the chosen method is in the list\n","if chosen_method not in model_list:\n","    print(\"Invalid method chosen!\")\n","else:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n","    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""],"metadata":{"id":"vNPL9WYg78l-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 허깅페이스 업로드\n","\n","모델을 허깅페이스로 업로드하려면 다음을 실행합니다.그러면 \"-GGUF\" 접미사가 붙은 새 리포지토리가 생성됩니다.다음 블록에서 `username` 변수를 변경하는 것을 잊지 마세요."],"metadata":{"id":"Ar8pO7bb80US"}},{"cell_type":"code","source":["!pip install -q huggingface_hub\n","\n","username = \"Redwood0\"\n","\n","from huggingface_hub import notebook_login, create_repo, HfApi\n","notebook_login()"],"metadata":{"id":"UOyKfUD-8jmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["api = HfApi()\n","\n","\n","create_repo(repo_id = f\"{username}/{MODEL_NAME}-GGUF\", repo_type=\"model\", exist_ok=True)\n","\n","api.upload_folder(\n","    folder_path=MODEL_NAME,\n","    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n","    allow_patterns=f\"*{GGML_VERSION}*\",\n",")"],"metadata":{"id":"ycBAGbQD8kgc"},"execution_count":null,"outputs":[]}]}